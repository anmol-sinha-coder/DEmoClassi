{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age-Gender-Race-Emotion-GPU.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "https://github.com/anmol-sinha-coder/DEmoClassi/blob/master/Age_Gender_Race_Emotion_GPU.ipynb",
      "authorship_tag": "ABX9TyNyxzWbHqYnB71ZUuVh4aN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmol-sinha-coder/DEmoClassi/blob/master/Age_Gender_Race_Emotion_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g--abrgzUbl3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/G_Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnLx-juX2pBA"
      },
      "source": [
        "! git clone https://github.com/anmol-sinha-coder/DEmoClassi.git\n",
        "! cp -ra DEmoClassi/{vision_utils,emotion_detection,multitask_rag,'setup.py'} ./\n",
        "! pip install tensorboardX pytorch-ignite pillow\n",
        "! unzip /content/G_Drive/MyDrive/ADNN/facial-expression-recognition-challenge.zip -d .\n",
        "! tar -xzvf /content/G_Drive/MyDrive/ADNN/UTKFace/UTKFace.tar.gz -C .\n",
        "! tar -xzvf /content/G_Drive/MyDrive/ADNN/UTKFace/crop_part1.tar.gz -C .\n",
        "! tar -xzvf fer2013.tar.gz\n",
        "! cp /content/G_Drive/MyDrive/ADNN/cv2_gpu/cv2.cpython-36m-x86_64-linux-gnu.so ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyilh9MmRaqB"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from vision_utils.custom_torch_utils import load_model\n",
        "from vision_utils.custom_architectures import SepConvModelMT, SepConvModel, initialize_model, PretrainedMT\n",
        "\n",
        "from emotion_detection.evaluate import evaluate_model as eval_fer\n",
        "from emotion_detection.fer_data_utils import *\n",
        "from emotion_detection.train import run_fer\n",
        "\n",
        "from multitask_rag.train import run_utk\n",
        "from multitask_rag.utk_data_utils import get_utk_dataloader, split_utk\n",
        "from multitask_rag.evaluate import evaluate_model as eval_utk\n",
        "from multitask_rag.utk_data_utils import display_examples_utk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import tqdm\n",
        "import random\n",
        "import cv2\n",
        "cv2.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFah_cf7nNx"
      },
      "source": [
        "## Fer2013 dataset\n",
        "Fer2013 is a kaggle dataset which consists of a set of 48x48 grayscale images representing the following facial expressions : \n",
        "* 0 : Angry\n",
        "* 1 : Disgust\n",
        "* 2 : Fear \n",
        "* 3 : Happy \n",
        "* 4 : Sad \n",
        "* 5 : Surprise \n",
        "* 6 : Neutral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKF7mvSQUT3F"
      },
      "source": [
        "path_fer = './fer2013/fer2013.csv'\n",
        "df_fer2013 = pd.read_csv(path_fer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIA39F-P4mYK"
      },
      "source": [
        "display_examples_fer(df_fer2013, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6lPU88E4oeY"
      },
      "source": [
        "display_examples_fer(df_fer2013, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1rir-Xl7Eyh"
      },
      "source": [
        "display_examples_fer(df_fer2013, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gehh2Xu97Tr1"
      },
      "source": [
        "display_examples_fer(df_fer2013, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23PMhdF_7XeA"
      },
      "source": [
        "display_examples_fer(df_fer2013, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzTTdxxa7cFU"
      },
      "source": [
        "display_examples_fer(df_fer2013, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOsVwgyz7gzs"
      },
      "source": [
        "display_examples_fer(df_fer2013, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwOH5aRq8KUe"
      },
      "source": [
        "## UTKFace dataset\n",
        "This is a dataset of cropped face images for the task of predicting the age, gender and race of a person.<br>\n",
        "\n",
        "**Age :** A number between 0 and 101 (representing the age of the person)<br>\n",
        "\n",
        "**Gender :**\n",
        "* 0 : Male\n",
        "* 1 : Female\n",
        "\n",
        "**Race :**\n",
        "* 0 : White\n",
        "* 1 : Black\n",
        "* 2 : Asian\n",
        "* 3 : Indian\n",
        "* 4 : Other\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_eWQCEV7imj"
      },
      "source": [
        "path_utk = './UTKFace/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9IIREl9odh"
      },
      "source": [
        "display_examples_utk(path_utk, 'gender', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lXMGe9L-HDd"
      },
      "source": [
        "display_examples_utk(path_utk, 'gender', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7uMJbmT9qTc"
      },
      "source": [
        "display_examples_utk(path_utk, 'race', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41kiL5qF9vdW"
      },
      "source": [
        "display_examples_utk(path_utk, 'race', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlAB1hrY90iC"
      },
      "source": [
        "display_examples_utk(path_utk, 'age', 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPHCrAPA-MZG"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now that we have the data ready, let's move to the funniest part : model training!\n",
        "As I have two separate datasets (`Fer2013` for emotion detection and `UTKFace` for gender-race-age prediction) we'll \n",
        "have to train two separate models. For each of the two tasks I tested 3 different architectures : \n",
        "* A CNN based on Depthwise Separable Convolution\n",
        "* Finetuning a pretrained Resnet50\n",
        "* Finetuning a pretrained VGG19\n",
        "\n",
        "<hr  size=10 color=black> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6_DbSPh6Uen"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device, torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R2O1XyQCXtJ"
      },
      "source": [
        "DATA_DIR = \"./fer2013/fer2013.csv\" # path to the csv file\n",
        "BATCH_SIZE = 256 # size of batches \n",
        "train_flag = 'Training' #`Usage` column in the csv file represents the usage of the data : train or validation or test\n",
        "val_flag = 'PublicTest'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_UgNyaqogxD"
      },
      "source": [
        "\n",
        "## (1)Training Emotion/Reaction detector\n",
        "### [1.a] Depthwise Separable Convolution model\n",
        "First we need to create DataLoader objects which are handy Pytorch objects for yielding batches of data during training.\n",
        "Basically, what the following code does is : \n",
        "* read the csv file and convert the raw pixels into numpy arrays\n",
        "* Apply some pre-processing operations : \n",
        "    * Histogram equalization\n",
        "    * Add a channel dimension so that the image becomes 48x48x1 instead of 48x48\n",
        "    * Convert the numpy array to a pytorch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wja9Wctg94b-"
      },
      "source": [
        "# The transformations to apply\n",
        "data_transforms = transforms.Compose([\n",
        "    HistEq(), # Apply histogram equalization\n",
        "    AddChannel(), # Add channel dimension to be able to apply convolutions\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader = get_fer_dataloader(BATCH_SIZE, DATA_DIR, train_flag, data_transforms=data_transforms)\n",
        "validation_dataloader = get_fer_dataloader(BATCH_SIZE, DATA_DIR, val_flag, data_transforms=data_transforms)\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': validation_dataloader\n",
        "}\n",
        "\n",
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/Separable_Convolutional_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)  # create the directory if it doesn't exist\n",
        "checkpoint = '/content/checkpoints/fer_sep_conv_histeq'  # folder where to save checkpoints during training\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CXvacNY_XG6"
      },
      "source": [
        "my_model = SepConvModel().to(device)\n",
        "my_optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4IP0kztGlk7"
      },
      "source": [
        "# Evaluation of model\n",
        "run_fer(model=my_model, optimizer=my_optimizer, epochs=300,\n",
        "        log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='FER-Sep_Conv',\n",
        "        n_saved=1, log_dir=None, launch_tensorboard=False,\n",
        "        patience=50, resume_model=None, resume_optimizer=None,\n",
        "        backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=0, n_cycle=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogF-GBWMsK4s"
      },
      "source": [
        "### [1.b] Resnet-50\n",
        "Over the years there is a trend to go more deeper, to solve more complex tasks and to also increase /improve the classification/recognition accuracy. But, as we go deeper; the training of neural network becomes difficult and also the accuracy starts saturating and then degrades also. Residual Learning tries to solve both these problems.\n",
        "\n",
        "- In general, in a deep convolutional neural network, several layers are stacked and are trained to the task at hand.\n",
        "- The network learns several low OR mid OR high level features at the end of its layers.\n",
        "- In residual learning, instead of trying to learn some features, we try to learn some residual. Residual can be simply understood as subtraction of feature learned from input of that layer.\n",
        "- ResNet does this using shortcut connections (directly connecting input of nth layer to some (n+x)th layer.\n",
        "- It has proved that training this form of networks is easier than training simple deep convolutional neural networks and also the problem of degrading accuracy is resolved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHxOdq9IG0QW"
      },
      "source": [
        "# The transformations to \n",
        "data_transforms = transforms.Compose([\n",
        "    HistEq(), # Apply histogram equalization\n",
        "    ToRGB(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': get_fer_dataloader(BATCH_SIZE, DATA_DIR, train_flag, data_transforms=data_transforms),\n",
        "    'valid': get_fer_dataloader(BATCH_SIZE, DATA_DIR, val_flag, data_transforms=data_transforms)\n",
        "}\n",
        "\n",
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/ResNet_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "checkpoint = '/content/checkpoints/fer_resnet_adam_histeq'\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivIuNwvsibFk"
      },
      "source": [
        "my_model, _ = initialize_model(model_name='resnet', feature_extract=True, num_classes=7, task='fer2013', use_pretrained=True, device=device)\n",
        "# The optimizer must only track the parameters that are trainable (thus excluding frozen ones)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCVPh91ok_4a"
      },
      "source": [
        "run_fer(model=my_model, optimizer=my_optimizer, epochs=200,\n",
        "        log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='FER-Resnet',\n",
        "        n_saved=1, log_dir=None, launch_tensorboard=False,\n",
        "        patience=75, val_monitor='acc', resume_model=None,\n",
        "        resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=10, n_cycle=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXnJFNrotwRx"
      },
      "source": [
        "### [1.c] VGG-19\n",
        "VGG19 is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer).\n",
        "\n",
        "- Conv3x3 (64)\n",
        "- Conv3x3 (64)\n",
        "- MaxPool\n",
        "- Conv3x3 (128)\n",
        "- Conv3x3 (128)\n",
        "- MaxPool\n",
        "- Conv3x3 (256)\n",
        "- Conv3x3 (256)\n",
        "- Conv3x3 (256)\n",
        "- Conv3x3 (256)\n",
        "- MaxPool\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- MaxPool\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- Conv3x3 (512)\n",
        "- MaxPool\n",
        "- Fully Connected (4096)\n",
        "- Fully Connected (4096)\n",
        "- Fully Connected (1000)\n",
        "- SoftMax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ2-8a_lr0BE"
      },
      "source": [
        "# The transformations to \n",
        "data_transforms = transforms.Compose([\n",
        "    HistEq(),\n",
        "    ToRGB(), \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': get_fer_dataloader(256, DATA_DIR, train_flag, data_transforms=data_transforms),\n",
        "    'valid': get_fer_dataloader(512, DATA_DIR, val_flag, data_transforms=data_transforms)\n",
        "}\n",
        "\n",
        "my_model, _ = initialize_model(model_name='vgg', feature_extract=True, num_classes=7, task='fer2013', use_pretrained=True, device=device)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3)\n",
        "\n",
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/VGGNet_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "checkpoint = '/content/checkpoints/fer_vgg_adam_histeq'\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Lxp79ItgI8"
      },
      "source": [
        "run_fer(model=my_model, optimizer=my_optimizer, epochs=300,\n",
        "        log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='FER-VGGnet',\n",
        "        n_saved=1, log_dir=None, launch_tensorboard=False,\n",
        "        patience=100, val_monitor='acc',\n",
        "        resume_model=None, resume_optimizer=None,\n",
        "        backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=20, n_cycle=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uzkfmdpxyX4"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# (2)Training Age, Gender, Race detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUab7N1g1Qqa"
      },
      "source": [
        "! mkdir logs/\n",
        "list_images = glob.glob('/content/UTKFace/*jp*')\n",
        "print(len(list_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPsL3jDc2S_H"
      },
      "source": [
        "# Labels are given in the image names. the image names format is the following : age_gender_race_date.\n",
        "# for instance this image name 1_0_0_20161219140623097.jpg.chip.jpg suggests that the image corresponds\n",
        "# to a person whose age is 1, gender is 0 (Male) and race is 0 (White). However there are few images for \n",
        "# which the name is malfomed, so we remove them using the following code snippet :\n",
        "# function to remove invalid images (that he filenames is not correctly formatted)\n",
        "def get_invalid_images(root_path='/content/data/UTKFace/'):\n",
        "    list_files = glob.glob(os.path.join(root_path, '*.[jJ][pP]*'))\n",
        "    filenames = [path.split('/')[-1].split('_') for path in list_files]\n",
        "    print()\n",
        "    invalid_images = []\n",
        "    for i, im in enumerate(tqdm.tqdm(filenames)):\n",
        "        if im[0].isdigit() and im[1].isdigit() and im[2].isdigit():\n",
        "            continue\n",
        "        else:\n",
        "            invalid_images.append(list_files[i])\n",
        "    return invalid_images\n",
        "\n",
        "\n",
        "invalid_images = get_invalid_images()\n",
        "print(invalid_images)\n",
        "for f in invalid_images: # removal of invalid images\n",
        "    os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "illW25ZHhxY2"
      },
      "source": [
        "## [2.a] Depthwise Separable Convolution model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEHh3YnT4C0e"
      },
      "source": [
        "# split the dataset into train, test and validation sets \n",
        "SRC_DIR = '/content/UTKFace/'  # path to the folder containing all images\n",
        "DEST_DIR = '/content/utk_split/' # path where to save the split dataset, 3 subdirectories will be created (train, valid and test)\n",
        "SPLIT = 0.7 # ratio of the train set, the remaining (30%) will be split equally between validation and test sets\n",
        "split_utk(SRC_DIR, DEST_DIR, SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcUzyRP84M8F"
      },
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader = get_utk_dataloader(batch_size=128, data_dir=DEST_DIR, data_transforms=data_transforms, flag='train')\n",
        "val_loader = get_utk_dataloader(batch_size=128, data_dir=DEST_DIR, data_transforms=data_transforms, flag='valid')\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': train_loader,\n",
        "    'valid': val_loader\n",
        "}\n",
        "\n",
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/Separable_Convolutional_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "checkpoint = '/content/checkpoints/utk_sep_conv_histeq'  # folder where to save checkpoints during training\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9GLlytf63Gr"
      },
      "source": [
        "my_model = SepConvModelMT(dropout=0.7, n_class=[1, 2, 5], n_filters=[64, 128, 256, 512], kernels_size=[3, 3, 3, 3]).to(device)\n",
        "my_optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3wrciXQc6W2"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300,\n",
        "        log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='UTK-Sep_Conv',\n",
        "        n_saved=1, log_dir='/content/logs', launch_tensorboard=False,\n",
        "        patience=50, resume_model=None, resume_optimizer=None,\n",
        "        backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=0, lr_after_freeze=None,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NchhSJ5LiTOU"
      },
      "source": [
        "## [2.b] ResNet-50 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3sjZrUgd0OZ"
      },
      "source": [
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/ResNet_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "checkpoint = '/content/checkpoints/utk_resnet_adam_histeq'\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9AfIr-kkqn"
      },
      "source": [
        "my_model = PretrainedMT(model_name='resnet', feature_extract=True, use_pretrained=True).to(device)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3hhdwu5ko_S"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300, log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='UTK-Resnet', n_saved=1,\n",
        "        log_dir='/content/logs', launch_tensorboard=False, patience=50,\n",
        "        resume_model=None, resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=10, n_cycle=None, lr_after_freeze=1e-4,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44], lr_plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6J1mc4l4LH"
      },
      "source": [
        "## [2.c] VGG-19 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7nl4O4El0ol"
      },
      "source": [
        "backup_path = '/content/G_Drive/MyDrive/ADNN/Demography_Psychology/VGGNet_Model'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "checkpoint = '/content/checkpoints/utk_vgg_adam_histeq'\n",
        "os.makedirs(checkpoint, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmn-EmZnT_U"
      },
      "source": [
        "my_model = PretrainedMT(model_name='vgg', feature_extract=True, use_pretrained=True).to(device)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iehIuf6eot5P"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300, log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint, filename_prefix='UTK-VGGnet', n_saved=1,\n",
        "        log_dir='/content/logs', launch_tensorboard=False, patience=50,\n",
        "        resume_model=None, resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=10, n_cycle=None, lr_after_freeze=1e-4,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44], lr_plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}