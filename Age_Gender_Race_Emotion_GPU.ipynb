{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age-Gender-Race-Emotion-GPU.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "https://github.com/anmol-sinha-coder/DEmoClassi/blob/master/Age_Gender_Race_Emotion_GPU.ipynb",
      "authorship_tag": "ABX9TyMhWOSvZvWxWZw/+MlvE1g8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmol-sinha-coder/DEmoClassi/blob/master/Age_Gender_Race_Emotion_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g--abrgzUbl3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/G_Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnLx-juX2pBA"
      },
      "source": [
        "! git clone https://github.com/anmol-sinha-coder/DEmoClassi.git\n",
        "! cp -ra DEmoClassi/{vision_utils,emotion_detection,multitask_rag,'setup.py'} ./\n",
        "! pip install tensorboardX pytorch-ignite pillow\n",
        "! unzip /content/G_Drive/MyDrive/ADNN/facial-expression-recognition-challenge.zip -d .\n",
        "! tar -xzvf /content/G_Drive/MyDrive/ADNN/UTKFace/UTKFace.tar.gz -C .\n",
        "! tar -xzvf fer2013.tar.gz\n",
        "! cp /content/G_Drive/MyDrive/ADNN/cv2_gpu/cv2.cpython-36m-x86_64-linux-gnu.so ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyilh9MmRaqB"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from vision_utils.custom_torch_utils import load_model\n",
        "from vision_utils.custom_architectures import SepConvModelMT, SepConvModel, initialize_model\n",
        "\n",
        "from emotion_detection.evaluate import evaluate_model as eval_fer\n",
        "from emotion_detection.fer_data_utils import *\n",
        "from emotion_detection.train import run_fer\n",
        "\n",
        "from multitask_rag.train import run_utk\n",
        "from multitask_rag.utk_data_utils import get_utk_dataloader\n",
        "from multitask_rag.evaluate import evaluate_model as eval_utk\n",
        "from multitask_rag.utk_data_utils import display_examples_utk\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "cv2.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFah_cf7nNx"
      },
      "source": [
        "## Fer2013 dataset\n",
        "Fer2013 is a kaggle dataset which consists of a set of 48x48 grayscale images representing the following facial expressions : \n",
        "* 0 : Angry\n",
        "* 1 : Disgust\n",
        "* 2 : Fear \n",
        "* 3 : Happy \n",
        "* 4 : Sad \n",
        "* 5 : Surprise \n",
        "* 6 : Neutral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKF7mvSQUT3F"
      },
      "source": [
        "path_fer = './fer2013/fer2013.csv'\n",
        "df_fer2013 = pd.read_csv(path_fer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIA39F-P4mYK"
      },
      "source": [
        "display_examples_fer(df_fer2013, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6lPU88E4oeY"
      },
      "source": [
        "display_examples_fer(df_fer2013, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1rir-Xl7Eyh"
      },
      "source": [
        "display_examples_fer(df_fer2013, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gehh2Xu97Tr1"
      },
      "source": [
        "display_examples_fer(df_fer2013, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23PMhdF_7XeA"
      },
      "source": [
        "display_examples_fer(df_fer2013, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzTTdxxa7cFU"
      },
      "source": [
        "display_examples_fer(df_fer2013, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOsVwgyz7gzs"
      },
      "source": [
        "display_examples_fer(df_fer2013, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwOH5aRq8KUe"
      },
      "source": [
        "## UTKFace dataset\n",
        "This is a dataset of cropped face images for the task of predicting the age, gender and race of a person.<br>\n",
        "\n",
        "**Age :** A number between 0 and 101 (representing the age of the person)<br>\n",
        "\n",
        "**Gender :**\n",
        "* 0 : Male\n",
        "* 1 : Female\n",
        "\n",
        "**Race :**\n",
        "* 0 : White\n",
        "* 1 : Black\n",
        "* 2 : Asian\n",
        "* 3 : Indian\n",
        "* 4 : Other\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_eWQCEV7imj"
      },
      "source": [
        "path_utk = './UTKFace/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9IIREl9odh"
      },
      "source": [
        "display_examples_utk(path_utk, 'gender', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lXMGe9L-HDd"
      },
      "source": [
        "display_examples_utk(path_utk, 'gender', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7uMJbmT9qTc"
      },
      "source": [
        "display_examples_utk(path_utk, 'race', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41kiL5qF9vdW"
      },
      "source": [
        "display_examples_utk(path_utk, 'race', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlAB1hrY90iC"
      },
      "source": [
        "display_examples_utk(path_utk, 'age', 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPHCrAPA-MZG"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now that we have the data ready, let's move to the funniest part : model training!\n",
        "As I have two separate datasets (`Fer2013` for emotion detection and `UTKFace` for gender-race-age prediction) we'll \n",
        "have to train two separate models. For each of the two tasks I tested 3 different architectures : \n",
        "* A CNN based on Depthwise Separable Convolution\n",
        "* Finetuning a pretrained Resnet50\n",
        "* Finetuning a pretrained VGG19\n",
        "\n",
        "<hr size=10 color=black>\n",
        "\n",
        "## Training emotion detector\n",
        "### a. Depthwise Separable Convolution model\n",
        "First we need to create DataLoader objects which are handy Pytorch objects for yielding batches of data during training.\n",
        "Basically, what the following code does is : \n",
        "* read the csv file and convert the raw pixels into numpy arrays\n",
        "* Apply some pre-processing operations : \n",
        "    * Histogram equalization ([see here for more information](https://en.wikipedia.org/wiki/Histogram_equalization))\n",
        "    * Add a channel dimension so that the image becomes 48x48x1 instead of 48x48\n",
        "    * Convert the numpy array to a pytorch tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wja9Wctg94b-"
      },
      "source": [
        "DATA_DIR = \"./fer2013/fer2013.csv\" # path to the csv file\n",
        "BATCH_SIZE = 256 # size of batches \n",
        "train_flag = 'Training' #`Usage` column in the csv file represents the usage of the data : train or validation or test\n",
        "val_flag = 'PublicTest'\n",
        "\n",
        "# The transformations to apply\n",
        "data_transforms = transforms.Compose([\n",
        "    HistEq(), # Apply histogram equalization\n",
        "    AddChannel(), # Add channel dimension to be able to apply convolutions\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader = get_fer_dataloader(BATCH_SIZE, DATA_DIR, train_flag, data_transforms=data_transforms)\n",
        "validation_dataloader = get_fer_dataloader(BATCH_SIZE, DATA_DIR, val_flag, data_transforms=data_transforms)\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': validation_dataloader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CXvacNY_XG6"
      },
      "source": [
        "my_model = SepConvModel()  # \n",
        "my_optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjp7ObADglQ"
      },
      "source": [
        "backup_path = '/content/G_Drive/MyDrive/ADNN/AgeGenderClassification/Training_Records/'\n",
        "os.makedirs(backup_path, exist_ok=True)  # create the directory if it doesn't exist\n",
        "checkpoint = '/content/checkpoints/sep_conv'  # folder where to save checkpoints during training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4IP0kztGlk7"
      },
      "source": [
        "# Evaluation of model\n",
        "run_fer(model=my_model,\n",
        "        optimizer=my_optimizer,\n",
        "        epochs=300,\n",
        "        log_interval=1,\n",
        "        dataloaders=my_data_loaders,\n",
        "        dirname=checkpoint,\n",
        "        n_saved=1,\n",
        "        log_dir=None,\n",
        "        launch_tensorboard=True,\n",
        "        patience=50,\n",
        "        resume_model=None,\n",
        "        resume_optimizer=None,\n",
        "        backup_step=5,\n",
        "        backup_path=backup_path,\n",
        "        n_epochs_freeze=0,\n",
        "        n_cycle=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHxOdq9IG0QW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}